Выводы:
INT8 выигрывает на малых батчах (234K vs 164K rps при batch=1)
На больших батчах FP32 и INT8 практически одинаковы (~12.5-13M rps)
CoreML/NPU медленнее CPU — модель слишком маленькая для Neural Engine
MPS (GPU) самый медленный — оверхед на передачу данных

Рекомендация для продакшена:
Latency-critical API → ONNX INT8
Batch processing → ONNX FP32 или INT8 (без разницы)

Поскольку тестирование проводилось на Apple Silicon, возможности сделать инференс на cuda не было, но скорее всего для такой
маленькой модели это бессмысленно.