# Результаты бенчмарков

Тестирование проводилось на Apple Silicon (M-series).

## Сводная таблица

### Batch size = 1 (latency-critical)

| Backend | Device | Latency (ms) | p95 (ms) | p99 (ms) | Throughput (rps) |
|---------|--------|--------------|----------|----------|------------------|
| **ONNX INT8** | CPU | **0.004** | 0.004 | 0.005 | **234,396** |
| ONNX FP32 | CPU | 0.006 | 0.018 | 0.030 | 163,901 |
| PyTorch | CPU | 0.012 | 0.012 | 0.015 | 86,263 |
| CoreML | ANE (NPU) | 0.015 | 0.019 | 0.040 | 64,714 |
| PyTorch | MPS (GPU) | 0.266 | 0.354 | 0.479 | 3,753 |

### Batch size = 256 (throughput-critical)

| Backend | Device | Latency (ms) | p95 (ms) | p99 (ms) | Throughput (rps) |
|---------|--------|--------------|----------|----------|------------------|
| **ONNX FP32** | CPU | **0.020** | 0.024 | 0.028 | **12,901,125** |
| ONNX INT8 | CPU | 0.020 | 0.022 | 0.022 | 12,572,883 |
| PyTorch | CPU | 0.031 | 0.033 | 0.039 | 8,329,369 |
| CoreML | ANE (NPU) | 0.047 | 0.054 | 0.084 | 5,423,967 |
| PyTorch | MPS (GPU) | 0.280 | 0.423 | 0.451 | 912,721 |

## Выводы

- **INT8 квантизация** даёт выигрыш на малых батчах (234K vs 164K rps)
- На больших батчах FP32 и INT8 практически одинаковы (~12.5-13M rps)
- **CoreML/NPU** медленнее CPU — модель слишком маленькая для Neural Engine
- **MPS (GPU)** самый медленный — оверхед на передачу данных превышает время вычислений

## Рекомендации для продакшена

| Сценарий | Рекомендация |
|----------|--------------|
| Latency-critical API | ONNX INT8 |
| Batch processing | ONNX FP32 или INT8 |
| Ограниченная память | ONNX INT8 (2x меньше размер) |

## Примечание

Тестирование на CUDA GPU не проводилось (Apple Silicon). Для модели такого размера (~16 KB) CUDA скорее всего не даст преимуществ из-за оверхеда на копирование данных CPU→GPU.
